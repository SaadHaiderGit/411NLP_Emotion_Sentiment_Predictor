#NLP Assignment 3 -- WSD.py

#This program is trained to learn the various sense of a word, then is tested with the Naive Bayes algorithm to determine which sense is being used.
#Each dataset is divided into five folds, with four used for training and one for test; this is done 5 times until all five folds have been tested.


#imports go here
import sys
import math
import string



def NBC(feature):
    '''Returns the probabilities for a sense given a feature, using Naive Bayes Classifier.'''
    p_s_given_f = []
    for i in range(len(sense_count)):
        if feature not in word_count[i]:
            word_count[i][feature] = 0
        p_f = (word_count[i][feature] + 1) / s_f_dem[i]         #p(F|S) --> [(count(feature) + 1) / (count(sense) + V)]
        p_s_given_f.append(p_f * sense_probs[i])                #p(S|F) --> p(F|S) * p(S)
    return p_s_given_f



def DLScore(num, dem):
    '''Returns the DL score of a feature word, showing how effective it is in determining the sense.'''
    return abs( math.log2(num/dem) )



#Main Program
if __name__ == "__main__":


    #Get corpus, then find senses
    #target_file = "plant.wsd"      #was used for debugging
    target_file = sys.argv[1]
    target_word = (target_file.split(".wsd"))[0]    #strip off ".wsd" to get the target word
    file = open(target_file)
    txt = [line for line in file if line != "\n"]   #get the entire corpus
    instance_num = int(len(txt)/6)                  #get the total number of instances

    sense = []                          #comb through the corpus to find each possible sense
    for i in range(instance_num):
        poss_sense = (txt[6*i+1].split('"'))[3]
        if poss_sense not in sense:
            sense.append(poss_sense)
        if len(sense) == 2:             #as only 2 senses are used for this project, stop once both senses are found
            break


    #Split all corpus instances into 5 folds. Each round, four are used for training, and one for test, so that all five folds are tested.
    r = instance_num % 5        #remainder. Used to find the number of instances per fold
    if r == 0:                  #(if no remainder, change the value to balance the below equations, so that fold_num = last_fold_num)
        r = 5
    fold_num = int( ((instance_num) - r + 5) / 5)   #equal num. of instances split evenly amongst the first four folds
    last_fold_num = int(fold_num - 5 + r)           #remaining instances given to fifth fold

    folds = []
    for m in range(4):          #for first four folds, add the appropriate instance information needed to calculate sense
        folds.append([])        
        for i in range(fold_num):
            instance_index = (fold_num * 6 * m) + (6 * i)                       #find the index of the instances we are pulling from in the text
            folds[m].append( (txt[instance_index+1], txt[instance_index+3]) )   #(only <answer instance...> and the actual sentence are needed for extraction purpose)
    
    folds.append([])            #for last fold, take the remaining instances and their info
    for i in range(last_fold_num):
        instance_index = (fold_num * 6 * 4) + (6 * i)
        folds[4].append( (txt[instance_index+1], txt[instance_index+3]) )        


    #----------------------------------------LOOP FOR USING FOLDS TO FIND WSD----------------------------------------
    out = open(target_word + ".wsd.out", "w")
    overall_corr_predictions = 0
    

    for m in range(5):  #each round a new fold becomes the test, and the rest are training; do this for 5 rounds so all folds are tested (with accuracy found for each)
        sense_count = [[x, 0] for x in sense]                   #counts for each individual sense
        total_sense_count = 0                                   #number of senses altogether
        if m == 4:                                              #(if last fold = test fold, subtract its instances from the total instances)
            total_sense_count = instance_num - last_fold_num
        else:                                                   #(else another fold = test fold; subtract its instances from the total instances)
            total_sense_count = instance_num - fold_num
        word_count = [{} for y in range(len(sense_count))]      #word counts correlate to each specific sense
        word_bank = []                                          #word bank contains all global words, regardless of sense

        #-----TRAIN-----
        for x in range(5):  #for each fold:
            if x == m:      #skip the test fold
                continue
            for s in range(len(folds[x])):
                curr_sense = (folds[x][s][0].split('"'))[3]         #take the sense of each fold
                for i in range(len(sense_count)):
                    if curr_sense == sense_count[i][0]:                #then increase the count of that sense
                        sense_count[i][1] += 1
                        sentence = folds[x][s][1]                   #also gather the words of the sentence for that sense
                        for words in sentence.lower().split():
                            if words != "<head>" + target_word + "</head>":     #(don't include the target word) 
                                if words[0] in string.punctuation:              #(strip off beginning and ending punctuation, for accurate counts)
                                    words = words[1:]
                                    if words == "":
                                        continue
                                if words[-1] in string.punctuation:                             
                                    words = words[:-1]
                                    if words == "":
                                        continue
                                if words not in word_bank:
                                    word_bank.append(words)
                                if words not in word_count[i]:                  #(if new word, add to dict; otherwise +1 to count)
                                    word_count[i][words] = 1
                                else:
                                    word_count[i][words] += 1
                        break;


        #apply Naive Bayes Model to find the DL score of each word and its correlated sense
        #(For this we create a list, then append to it each word with its score and appropriate tag)
        DL_list = []
        sense_probs = []
        s_f_dem = []                            #aka the demonimators for a feature-given-sense in Naive Bayes; w/ add-one smoothing, is (sense_count + sense vocab)
        for i in range(len(sense_count)):       #store the probability of each sense appearing, then the demonimators
            sense_probs.append(sense_count[i][1] / total_sense_count)
            s_f_dem.append(sense_count[i][1] + len(word_bank))

        for word in word_bank:
            f_given_s_probs = NBC(word)                     #use Naive Bayes Classifier, then apply sense with higher prob. to the current word
            if f_given_s_probs[0] > f_given_s_probs[1]:
                DL_list.append([ word, sense[0], DLScore(f_given_s_probs[0], f_given_s_probs[1]) ])
            else:
                DL_list.append([ word, sense[1], DLScore(f_given_s_probs[1], f_given_s_probs[0]) ])

        DL_list = sorted(DL_list, key = lambda x: -x[2])    #sort DL_list by its highest scores
        if sense_probs[0] > sense_probs[1]:                 #if the DL_list fails to predict the sense, use the higher prob. sense as the tiebreaker)
            DL_list.append(["<TIEBREAK \>", sense[0], 0])
        else:
            DL_list.append(["<TIEBREAK \>", sense[1], 0])


        #-----TEST-----
        out.write("Fold " + str(m+1) + "\n")
        corr_predictions = 0
        predictions = len(folds[m])         #the number of instances in the test fold = the number of predictions made 

        for s in range(len(folds[m])):      #for each test fold instance, pull out its sentence and predict it with DL_list
            id = (folds[m][s][0].split('"'))[1]
            ground_truth = (folds[m][s][0].split('"'))[3]
            sentence = folds[m][s][1].lower().split()
            for i, words in enumerate(sentence):
                if words != "<head>" + target_word + "</head>":     #(don't include the target word) 
                    if words[0] in string.punctuation:              #(strip off beginning and ending punctuation, for accurate predictions)
                        sentence[i] = words[1:]
                        if words == "":
                            continue
                    if words[-1] in string.punctuation:                             
                        sentence[i] = words[:-1]

            for i in DL_list:                                       #Predict the sentence. Either a feature word is found, or a tiebreaker is needed.
                if i[0] in sentence or i[0] == "<TIEBREAK \>":
                    if i[1] == ground_truth:                        #If the feature's associated sense matches the ground truth, increment corr_predictions
                        corr_predictions += 1
                    out.write(id + " " + i[1] + "\n")
                    break;

        fold_acc = corr_predictions / predictions * 100                         #output the fold's accuracy
        print("Accuracy of fold " + str(m+1) + ":", round(fold_acc, 1), "%")     
        overall_corr_predictions += corr_predictions                            #add up the number of correct predictions for later
        out.write("\n")


    #The loop has ended. Find the total accuracy (and close the output).
    overall_fold_acc = overall_corr_predictions / instance_num * 100            #(instance_num is all of the instances tested in this file)
    print("Total accuracy:", round(overall_fold_acc, 1), "%")
    out.close()
